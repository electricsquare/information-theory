<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Tom Read Cutting">
  <title>Information Theory</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reset.css">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Information Theory</h1>
  <p class="author">Tom Read Cutting</p>
</section>

<section id="what-is" class="slide level3">
<h3>What Is?</h3>
<p>Understanding the movement and transformation of information through mathematical and physical laws, addressing and answering two fundamental questions:</p>
<ol type="1">
<li><p>How can much can you compress data? (The entropy of the data, H).</p></li>
<li><p>At which rate can you reliable communicate through a channel? (The channel capacity, C).</p></li>
</ol>
</section>
<section id="why-do" class="slide level3">
<h3>Why Do?</h3>
<p>Widely Applicable! (Sneak Peak):</p>
<ul>
<li>Compression (duh!)</li>
<li>Communications and Networking (duh!)</li>
<li>Data Oriented Design</li>
<li>Security</li>
<li>Machine Learning (huh?)</li>
<li>Compute Vision (huh?)</li>
<li>Computer Graphics (huh!?)</li>
<li>^^ Almost Everything in Computer Science</li>
</ul>
<aside class="notes">
<p>Information Theory has some pretty obvious applications, but hopefully some here will surprise you!</p>
<p>We will be delving into the more interesting links later</p>
</aside>
</section>
<section id="what-contains" class="slide level3">
<h3>What Contains?</h3>
<ul>
<li>Foundations: Intro, Bayes, Entropy, Shannon’s Theorems</li>
<li>Applications: Codes, Sampling, Compression</li>
<li>Relations: DoD, Security, ML, Graphics</li>
</ul>
<p><em>Not</em> a mathematically rigorous presentation! Arguments rely on intuition, <em>not</em> formal proof. User-friendly <em>not</em> technically precise.</p>
</section>
<section>
<section id="foundations" class="title-slide slide level2">
<h2>Foundations</h2>

</section>
<section id="data-is-not-information" class="slide level3">
<h3>Data Is Not Information</h3>
<p>Intuition: A new hard drive has 1,000,000,000,000 bits of data, but not 1,000,000,000,000.</p>
<p>Is there a difference between a 0-initialized hard drive, and a randomly-initialized hard drive in terms of information?</p>
</section>
<section id="probabilities-matter" class="slide level3">
<h3>Probabilities Matter</h3>
<p>The less probable an event is, the more information it contains when it happens.</p>
</section>
<section id="bit-of-data-vs-1-bit-of-information" class="slide level3">
<h3>1 Bit of Data vs 1 Bit of Information</h3>
<p>We can say that 1 Bit of Data contains 1 Bit of Information if the probability of that Bit being 1 or 0 is 0.5.</p>
</section>
<section id="knowledge-affects-information" class="slide level3">
<h3>Knowledge Affects Information</h3>
<p>Intuitively, past events affect the probabilities by which we predict future events.</p>
<p>In othr wrds, yo cn rd ths sntnce evn wth mssng lttrs.</p>
</section></section>
<section>
<section id="some-probability" class="title-slide slide level2">
<h2>Some Probability</h2>

</section>
<section id="product-rule" class="slide level3">
<h3>Product Rule</h3>
<p>The probability that both <em>A</em> and <em>B</em> will happen:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>B</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>B</mi><mo stretchy="false" form="prefix">|</mo><mi>A</mi><mo stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(A,B) = p(A|B)p(B) = p(B|A)p(A)</annotation></semantics></math></p>
<p>Example: The Probability that Alice will buy a hot dog <em>and</em> ketchup?</p>
<aside class="notes">
<p>If we know the probability of Alice buying ketchup given that she’s bought a hot dog. <em>And</em> we know how likely she is to buy a hot dog. Then we know how likely <em>both</em> are to happen.</p>
</aside>
</section>
<section id="sum-rule" class="slide level3">
<h3>Sum Rule</h3>
<p>If the probability of A is affected by the outcome of a number of events <em>B</em></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>∑</mo><mi>B</mi></munder><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo>,</mo><mi>B</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><munder><mo>∑</mo><mi>B</mi></munder><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="false" form="postfix">)</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>B</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">p(A) = \sum\limits_{B} p(A,B) = \sum_{B} p(A|B)p(B)</annotation></semantics></math></p>
<p>Example: The Probability that Bob will beat Alice at chess.</p>
<aside class="notes">
<p>If we know the probability that Bob will beat Alice when starting with whites, and the probability that Bob will beat Alice when starting with blacks - then we know the probability that Bob will beat Alice if we know how likely he is to start with either of those colours.</p>
</aside>
</section>
<section id="bayes-theorem" class="slide level3">
<h3>Bayes’ Theorem</h3>
<p>Super-duper important, we aren’t going to derive it, but have a look at this majestic thing:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>B</mi><mo stretchy="false" form="prefix">|</mo><mi>A</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="prefix">|</mo><mi>B</mi><mo stretchy="false" form="postfix">)</mo><mo stretchy="false" form="prefix">(</mo><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>B</mi><mo stretchy="false" form="postfix">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false" form="prefix">(</mo><mi>A</mi><mo stretchy="false" form="postfix">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">p(B|A) = \frac{p(A|B)(p(B)}{p(A)}</annotation></semantics></math></p>
<aside class="notes">
<p>Derived from the product and sum rules.</p>
</aside>
</section>
<section id="relation-to-information-theory" class="slide level3">
<h3>Relation to Information Theory</h3>
<p>Bayes’ Theorem can be applied recursively to let us use the latest posterior as a new <em>prior</em> so interpret the next set of data.</p>
<p>Information Theory is about quantitatively analysing the amount of information gained (via analysing reduced uncertainty) using Bayes’ Theorem.</p>
</section></section>
<section>
<section id="entropy" class="title-slide slide level2">
<h2>Entropy</h2>

</section>
<section class="slide level3">

<p>The information <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>I</mi><annotation encoding="application/x-tex">I</annotation></semantics></math> contained within an event is:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo>=</mo><msub><mo>log</mo><mn>2</mn></msub><mo stretchy="false" form="prefix">(</mo><mi>p</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">I = \log_2(p)</annotation></semantics></math></p>
<p>Where <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> is the probability of that event occurring.</p>
<p>Entropy, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mo>−</mo><mi>I</mi></mrow><annotation encoding="application/x-tex">H = -I</annotation></semantics></math> is the the amount of uncertainty.</p>
</section>
<section id="adding-information" class="slide level3">
<h3>Adding Information</h3>
<p>For independent <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math> and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>I</mi><mrow><mi>a</mi><mi>b</mi></mrow></msub><mo>=</mo><msub><mo>log</mo><mn>2</mn></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>p</mi><mi>a</mi></msub><msub><mi>p</mi><mi>b</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mo>log</mo><mn>2</mn></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>p</mi><mi>a</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>+</mo><msub><mo>log</mo><mn>2</mn></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>p</mi><mi>b</mi></msub><mo stretchy="false" form="postfix">)</mo><mo>=</mo><msub><mi>I</mi><mi>a</mi></msub><mo>+</mo><msub><mi>I</mi><mi>b</mi></msub></mrow><annotation encoding="application/x-tex">I_{ab} = \log_2(p_a p_b) = \log_2(p_a) + \log_2(p_b) = I_a + I_b</annotation></semantics></math></p>
<aside class="notes">
<p>By defining information in terms of the logarithms of the underlying probabilities involved, we can “add” information together to get the total information gain of two events.</p>
</aside>
</section>
<section id="entropy-of-ensembles" class="slide level3">
<h3>Entropy of Ensembles</h3>
<p>If you have non-uniform ensemble of probabilities such that:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>p</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sum\limits_i p_i = 1 </annotation></semantics></math></p>
<p>Then:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mo>−</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>p</mi><mi>i</mi></msub><msub><mo>log</mo><mn>2</mn></msub><mo stretchy="false" form="prefix">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">H = - \sum\limits_i p_i \log_2(p_i)</annotation></semantics></math></p>
<aside class="notes">
<p>TODO: Intuition</p>
</aside>
</section>
<section id="intuition" class="slide level3">
<h3>Intuition</h3>
<p>Bit, <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">b=1</annotation></semantics></math> with probability <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> (and <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math> otherwise):</p>
<figure>
<img data-src="plots/-7006491044744867850.png" class="matplotlib" height="200" alt="" /><figcaption><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false" form="prefix">(</mo><mi>p</mi><mo stretchy="false" form="postfix">)</mo><mo>=</mo><mo>−</mo><mi>p</mi><msub><mo>log</mo><mn>2</mn></msub><mo stretchy="false" form="prefix">(</mo><mi>p</mi><mo stretchy="false" form="postfix">)</mo><mo>−</mo><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy="false" form="postfix">)</mo><msub><mo>log</mo><mn>2</mn></msub><mo stretchy="false" form="prefix">(</mo><mn>1</mn><mo>−</mo><mi>p</mi><mo stretchy="false" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">H(p)=-p\log_2(p)-(1-p)\log_2(1-p)</annotation></semantics></math></figcaption>
</figure>
<p>When <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">p=0.5</annotation></semantics></math>, the Entropy maxes-out at 1.</p>
<aside class="notes">
<p>Bring that back to earlier when we said that 1 bit of data contains 1 bit of information if the probability of it being 1 or 0 was 0.5, this is why!</p>
</aside>
</section>
<section id="more" class="slide level3">
<h3>More</h3>
<ul>
<li>Joint Entropy</li>
<li>Conditional Entropy of Ensembles</li>
<li>Chain Rule for Entropy</li>
<li>Mutual Information</li>
<li>Kullback-Leibler Distance and Fano’s Inequality</li>
</ul>
</section></section>
<section>
<section id="source-coding" class="title-slide slide level2">
<h2>Source Coding</h2>

</section>
<section class="slide level3">

<p>We can model a stream of symbols as a “Markov Process”.</p>
</section></section>
<section>
<section id="the-end" class="title-slide slide level2">
<h2>The End</h2>

</section>
<section id="references" class="slide level3">
<h3>References</h3>
<div id="refs" class="references hanging-indent" role="doc-bibliography">
<div id="ref-DaugmanJohn2016">
<p>Daugman, John. 2016. “Information Theory.” <a href="https://www.cl.cam.ac.uk/teaching/1617/InfoTheory/materials.html">https://www.cl.cam.ac.uk/teaching/1617/InfoTheory/materials.html</a>.</p>
</div>
</div>
</section></section>
    </div>
  </div>

  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
